{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pennylane as qml\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import pandas as pd\n",
    "import argparse\n",
    "import os\n",
    "import math\n",
    "import datetime\n",
    "import time\n",
    "from frechetdist import frdist\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torch.utils.data as data\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "from solver import Solver\n",
    "from data_loader import get_loader\n",
    "from torch.backends import cudnn\n",
    "from utils import *\n",
    "from models import Generator, Discriminator\n",
    "from data.sparse_molecular_dataset import SparseMolecularDataset\n",
    "from rdkit import Chem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def str2bool(v):\n",
    "    return v.lower() in ('true')\n",
    "\n",
    "qubits = 8\n",
    "# Set up your ibmq credentials first from https://quantum-computing.ibm.com/\n",
    "demo_on_ibmq = False\n",
    "\n",
    "if demo_on_ibmq:\n",
    "    dev = qml.device('qiskit.ibmq', wires=qubits, backend='ibmq_16_melbourne')\n",
    "else:\n",
    "    dev = qml.device('default.qubit', wires=qubits)\n",
    "\n",
    "@qml.qnode(dev, interface='torch')\n",
    "def gen_circuit(w):\n",
    "    # random noise as generator input\n",
    "    z1 = random.uniform(-1, 1)\n",
    "    z2 = random.uniform(-1, 1)\n",
    "    layers = 1    \n",
    "    \n",
    "    # construct generator circuit for both atom vector and node matrix\n",
    "    for i in range(qubits):\n",
    "        qml.RY(np.arcsin(z1), wires=i)\n",
    "        qml.RZ(np.arcsin(z2), wires=i)\n",
    "        \n",
    "    for l in range(layers):\n",
    "        for i in range(qubits):\n",
    "            qml.RY(w[i], wires=i)\n",
    "        for i in range(qubits-1):\n",
    "            qml.CNOT(wires=[i, i+1])\n",
    "            qml.RZ(w[i+qubits], wires=i+1)\n",
    "            qml.CNOT(wires=[i, i+1])\n",
    "    return [qml.expval(qml.PauliZ(i)) for i in range(qubits)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(quantum=True, patches=1, layer=1, qubits=8, z_dim=8, g_conv_dim=[128], d_conv_dim=[[128, 64], 128, [128, 64]], g_repeat_num=6, d_repeat_num=6, lambda_cls=1, lambda_rec=10, lambda_gp=10, post_method='softmax', batch_size=16, num_iters=5000, num_iters_decay=2500, g_lr=0.0001, d_lr=0.0001, dropout=0.0, n_critic=5, beta1=0.5, beta2=0.999, resume_iters=None, test_iters=5000, num_workers=1, mode='train', use_tensorboard=False, mol_data_dir='data/gdb9_9nodes.sparsedataset', log_dir='qgan-hg/logs', model_save_dir='qgan-hg/models', sample_dir='qgan-hg/samples', result_dir='qgan-hg/results', log_step=10, sample_step=1000, model_save_step=1000, lr_update_step=500)\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "# Quantum circuit configuration\n",
    "parser.add_argument('--quantum', type=bool, default=True, help='choose to use quantum gan with hybrid generator')\n",
    "parser.add_argument('--patches', type=int, default=1, help='number of quantum circuit patches')\n",
    "parser.add_argument('--layer', type=int, default=1, help='number of repeated variational quantum layer')\n",
    "parser.add_argument('--qubits', type=int, default=8, help='number of qubits and dimension of domain labels')\n",
    "\n",
    "# Model configuration.\n",
    "parser.add_argument('--z_dim', type=int, default=8, help='dimension of domain labels')\n",
    "parser.add_argument('--g_conv_dim', default=[128], help='number of conv filters in the first layer of G')\n",
    "parser.add_argument('--d_conv_dim', type=int, default=[[128, 64], 128, [128, 64]], help='number of conv filters in the first layer of D')\n",
    "parser.add_argument('--g_repeat_num', type=int, default=6, help='number of residual blocks in G')\n",
    "parser.add_argument('--d_repeat_num', type=int, default=6, help='number of strided conv layers in D')\n",
    "parser.add_argument('--lambda_cls', type=float, default=1, help='weight for domain classification loss')\n",
    "parser.add_argument('--lambda_rec', type=float, default=10, help='weight for reconstruction loss')\n",
    "parser.add_argument('--lambda_gp', type=float, default=10, help='weight for gradient penalty')\n",
    "parser.add_argument('--post_method', type=str, default='softmax', choices=['softmax', 'soft_gumbel', 'hard_gumbel'])\n",
    "\n",
    "# Training configuration.\n",
    "parser.add_argument('--batch_size', type=int, default=16, help='mini-batch size')\n",
    "parser.add_argument('--num_iters', type=int, default=5000, help='number of total iterations for training D')\n",
    "parser.add_argument('--num_iters_decay', type=int, default=2500, help='number of iterations for decaying lr')\n",
    "parser.add_argument('--g_lr', type=float, default=0.0001, help='learning rate for G')\n",
    "parser.add_argument('--d_lr', type=float, default=0.0001, help='learning rate for D')\n",
    "parser.add_argument('--dropout', type=float, default=0., help='dropout rate')\n",
    "parser.add_argument('--n_critic', type=int, default=5, help='number of D updates per each G update')\n",
    "parser.add_argument('--beta1', type=float, default=0.5, help='beta1 for Adam optimizer')\n",
    "parser.add_argument('--beta2', type=float, default=0.999, help='beta2 for Adam optimizer')\n",
    "parser.add_argument('--resume_iters', type=int, default=None, help='resume training from this step')\n",
    "\n",
    "# Test configuration.\n",
    "parser.add_argument('--test_iters', type=int, default=5000, help='test model from this step')\n",
    "\n",
    "# Miscellaneous.\n",
    "parser.add_argument('--num_workers', type=int, default=1)\n",
    "parser.add_argument('--mode', type=str, default='train', choices=['train', 'test'])\n",
    "parser.add_argument('--use_tensorboard', type=str2bool, default=False)\n",
    "\n",
    "# Directories.\n",
    "parser.add_argument('--mol_data_dir', type=str, default='data/gdb9_9nodes.sparsedataset')\n",
    "parser.add_argument('--log_dir', type=str, default='qgan-hg/logs')\n",
    "parser.add_argument('--model_save_dir', type=str, default='qgan-hg/models')\n",
    "parser.add_argument('--sample_dir', type=str, default='qgan-hg/samples')\n",
    "parser.add_argument('--result_dir', type=str, default='qgan-hg/results')\n",
    "\n",
    "# Step size.\n",
    "parser.add_argument('--log_step', type=int, default=10)\n",
    "parser.add_argument('--sample_step', type=int, default=1000)\n",
    "parser.add_argument('--model_save_step', type=int, default=1000)\n",
    "parser.add_argument('--lr_update_step', type=int, default=500)\n",
    "\n",
    "\n",
    "config = parser.parse_known_args()[0]\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator(\n",
      "  (layers): Sequential(\n",
      "    (0): Linear(in_features=8, out_features=128, bias=True)\n",
      "    (1): Tanh()\n",
      "    (2): Dropout(p=0.0, inplace=True)\n",
      "  )\n",
      "  (edges_layer): Linear(in_features=128, out_features=405, bias=True)\n",
      "  (nodes_layer): Linear(in_features=128, out_features=54, bias=True)\n",
      "  (dropoout): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "G\n",
      "The number of parameters: 60363\n",
      "Discriminator(\n",
      "  (gcn_layer): GraphConvolution(\n",
      "    (linear1): Linear(in_features=6, out_features=128, bias=True)\n",
      "    (linear2): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (agg_layer): GraphAggregation(\n",
      "    (sigmoid_linear): Sequential(\n",
      "      (0): Linear(in_features=69, out_features=128, bias=True)\n",
      "      (1): Sigmoid()\n",
      "    )\n",
      "    (tanh_linear): Sequential(\n",
      "      (0): Linear(in_features=69, out_features=128, bias=True)\n",
      "      (1): Tanh()\n",
      "    )\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (linear_layer): Sequential(\n",
      "    (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (1): Dropout(p=0.0, inplace=False)\n",
      "    (2): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (3): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (output_layer): Linear(in_features=64, out_features=1, bias=True)\n",
      ")\n",
      "D\n",
      "The number of parameters: 51905\n"
     ]
    }
   ],
   "source": [
    "self = Solver(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the trained models from step 5000...\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'qgan-hg/models/5000-G.ckpt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresume_iters:\n\u001b[1;32m      7\u001b[0m     start_iters \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresume_iters\n\u001b[0;32m----> 8\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrestore_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresume_iters\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m     gen_weights \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.11443097\u001b[39m,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.23893048\u001b[39m,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.26079974\u001b[39m,\u001b[38;5;241m0.52572775\u001b[39m,\u001b[38;5;241m0.04154618\u001b[39m,\u001b[38;5;241m0.7797117\u001b[39m,\n\u001b[1;32m     10\u001b[0m                                 \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.22719051\u001b[39m,\u001b[38;5;241m0.04173521\u001b[39m,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.7405998\u001b[39m,\u001b[38;5;241m0.040963333\u001b[39m,\u001b[38;5;241m0.13625668\u001b[39m,\u001b[38;5;241m0.5491951\u001b[39m,\u001b[38;5;241m0.41576374\u001b[39m,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.059020802\u001b[39m,\u001b[38;5;241m0.7136884\u001b[39m], requires_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     11\u001b[0m ibm_sample_list \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/Downloads/TRIAL/quantum-gan-as-per-pp2/qgan-asq/solver.py:151\u001b[0m, in \u001b[0;36mSolver.restore_model\u001b[0;34m(self, resume_iters)\u001b[0m\n\u001b[1;32m    149\u001b[0m D_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_save_dir, \u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m-D.ckpt\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(resume_iters))\n\u001b[1;32m    150\u001b[0m V_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_save_dir, \u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m-V.ckpt\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(resume_iters))\n\u001b[0;32m--> 151\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mG\u001b[38;5;241m.\u001b[39mload_state_dict(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mG_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloc\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mD\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(D_path, map_location\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m storage, loc: storage))\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mV\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(V_path, map_location\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m storage, loc: storage))\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/fyp_env/lib/python3.10/site-packages/torch/serialization.py:998\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m    995\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    996\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 998\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[1;32m    999\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m   1000\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m   1001\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m   1002\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m   1003\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/fyp_env/lib/python3.10/site-packages/torch/serialization.py:445\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    444\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 445\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    446\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    447\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/fyp_env/lib/python3.10/site-packages/torch/serialization.py:426\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[0;32m--> 426\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'qgan-hg/models/5000-G.ckpt'"
     ]
    }
   ],
   "source": [
    "# Inference of generated molecules\n",
    "start_iters = 0\n",
    "# load the 5000 checkpoint (replace earlier 2580)\n",
    "self.resume_iters = 5000\n",
    "\n",
    "if self.resume_iters:\n",
    "    start_iters = self.resume_iters\n",
    "    self.restore_model(self.resume_iters)\n",
    "    gen_weights = torch.tensor([-0.11443097,-0.23893048,-0.26079974,0.52572775,0.04154618,0.7797117,\n",
    "                                -0.22719051,0.04173521,-0.7405998,0.040963333,0.13625668,0.5491951,0.41576374,-0.059020802,0.7136884], requires_grad=True)\n",
    "ibm_sample_list = []\n",
    "for i in range(self.batch_size):\n",
    "    # Running time depends on the queue of IBM melbourne machine\n",
    "    if demo_on_ibmq:\n",
    "        print(\"IBM Q running job {}/{}\".format(i+1, self.batch_size), end=\"\\r\")\n",
    "    ibm_sample_list.append(gen_circuit(gen_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start inference.\n",
    "print('Start inference...')\n",
    "start_time = time.time()\n",
    "\n",
    "mols, _, _, a, x, _, _, _, _ = self.data.next_train_batch(self.batch_size)\n",
    "\n",
    "# =================================================================================== #\n",
    "#                             1. Preprocess input data                                #\n",
    "# =================================================================================== #\n",
    "\n",
    "a = torch.from_numpy(a).to(self.device).long()            # Adjacency.\n",
    "x = torch.from_numpy(x).to(self.device).long()            # Nodes.\n",
    "a_tensor = self.label2onehot(a, self.b_dim)\n",
    "x_tensor = self.label2onehot(x, self.m_dim)\n",
    "z = torch.stack(tuple(ibm_sample_list)).to(self.device).float()\n",
    "\n",
    "# Z-to-target\n",
    "edges_logits, nodes_logits = self.G(z)\n",
    "# Postprocess with Gumbel softmax\n",
    "(edges_hat, nodes_hat) = self.postprocess((edges_logits, nodes_logits), self.post_method)\n",
    "logits_fake, features_fake = self.D(edges_hat, None, nodes_hat)\n",
    "g_loss_fake = - torch.mean(logits_fake)\n",
    "\n",
    "# Real Reward\n",
    "rewardR = torch.from_numpy(self.reward(mols)).to(self.device)\n",
    "# Fake Reward\n",
    "(edges_hard, nodes_hard) = self.postprocess((edges_logits, nodes_logits), 'hard_gumbel')\n",
    "edges_hard, nodes_hard = torch.max(edges_hard, -1)[1], torch.max(nodes_hard, -1)[1]\n",
    "mols = [self.data.matrices2mol(n_.data.cpu().numpy(), e_.data.cpu().numpy(), strict=True)\n",
    "        for e_, n_ in zip(edges_hard, nodes_hard)]\n",
    "rewardF = torch.from_numpy(self.reward(mols)).to(self.device)\n",
    "\n",
    "# Value loss\n",
    "value_logit_real,_ = self.V(a_tensor, None, x_tensor, torch.sigmoid)\n",
    "value_logit_fake,_ = self.V(edges_hat, None, nodes_hat, torch.sigmoid)\n",
    "g_loss_value = torch.mean((value_logit_real - rewardR) ** 2 + (\n",
    "                           value_logit_fake - rewardF) ** 2)\n",
    "\n",
    "R=[list(a[i].reshape(-1))  for i in range(self.batch_size)]\n",
    "F=[list(edges_hard[i].reshape(-1))  for i in range(self.batch_size)]\n",
    "fd_bond_only = frdist(R, F)\n",
    "\n",
    "R=[list(x[i]) + list(a[i].reshape(-1))  for i in range(self.batch_size)]\n",
    "F=[list(nodes_hard[i]) + list(edges_hard[i].reshape(-1))  for i in range(self.batch_size)]\n",
    "fd_bond_atom = frdist(R, F)\n",
    "\n",
    "loss = {}\n",
    "loss['G/loss_fake'] = g_loss_fake.item()\n",
    "loss['G/loss_value'] = g_loss_value.item()\n",
    "loss['FD/fd_bond_only'] = fd_bond_only\n",
    "loss['FD/fd_bond_atom'] = fd_bond_atom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Print out training information.\n",
    "et = time.time() - start_time\n",
    "et = str(datetime.timedelta(seconds=et))[:-7]\n",
    "log = \"Elapsed [{}], Iteration [{}/{}]\".format(et, start_iters, self.num_iters)\n",
    "\n",
    "# Log update\n",
    "m0, m1 = all_scores(mols, self.data, norm=True)     # 'mols' is output of Fake Reward\n",
    "m0 = {k: np.array(v)[np.nonzero(v)].mean() for k, v in m0.items()}\n",
    "m0.update(m1)\n",
    "loss.update(m0)\n",
    "for tag, value in loss.items():\n",
    "    log += \", {}: {:.4f}\".format(tag, value)\n",
    "print(log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only valid moleculues evaluated by RDKit\n",
    "valid_mols = [i for i in mols if i != None]\n",
    "\n",
    "from rdkit.Chem.Draw import SimilarityMaps\n",
    "import matplotlib\n",
    "\n",
    "for mol in valid_mols:\n",
    "    AllChem.ComputeGasteigerCharges(mol)\n",
    "    contribs = [mol.GetAtomWithIdx(i).GetDoubleProp('_GasteigerCharge') for i in range(mol.GetNumAtoms())]\n",
    "    fig = SimilarityMaps.GetSimilarityMapFromWeights(mol, contribs, colorMap=None,  contourLines=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fyp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
